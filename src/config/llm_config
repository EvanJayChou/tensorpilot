# src/config/llm_config.py
from pydantic_settings import BaseSettings
from typing import Optional
import openai
from langchain_openai import AzureChatOpenAI
from langchain_core.language_models.chat_models import BaseChatModel
import logging

logger = logging.getLogger(__name__)

class AzureOpenAISettings(BaseSettings):
    # Azure OpenAI Configuration
    azure_openai_api_key: str
    azure_openai_endpoint: str
    azure_openai_api_version: str = "2024-02-15-preview"
    azure_openai_deployment_name: str = "gpt-4"  # Your deployment name
    azure_openai_model_name: str = "gpt-4"
    
    # Model Parameters
    temperature: float = 0.1
    max_tokens: Optional[int] = 4000
    top_p: float = 0.95
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0
    
    class Config:
        env_file = ".env"
        env_prefix = ""

class LLMManager:
    """Manages Azure OpenAI LLM instances"""
    
    def __init__(self):
        self.settings = AzureOpenAISettings()
        self._llm = None
        self._initialize_llm()
    
    def _initialize_llm(self) -> None:
        """Initialize the Azure OpenAI LLM"""
        try:
            # Configure OpenAI client for direct API calls if needed
            openai.api_type = "azure"
            openai.api_key = self.settings.azure_openai_api_key
            openai.api_base = self.settings.azure_openai_endpoint
            openai.api_version = self.settings.azure_openai_api_version
            
            # Create LangChain Azure OpenAI instance
            self._llm = AzureChatOpenAI(
                azure_endpoint=self.settings.azure_openai_endpoint,
                api_key=self.settings.azure_openai_api_key,
                api_version=self.settings.azure_openai_api_version,
                deployment_name=self.settings.azure_openai_deployment_name,
                model=self.settings.azure_openai_model_name,
                temperature=self.settings.temperature,
                max_tokens=self.settings.max_tokens,
                top_p=self.settings.top_p,
                frequency_penalty=self.settings.frequency_penalty,
                presence_penalty=self.settings.presence_penalty,
            )
            
            logger.info(f"Azure OpenAI LLM initialized with deployment: {self.settings.azure_openai_deployment_name}")
            
        except Exception as e:
            logger.error(f"Failed to initialize Azure OpenAI LLM: {str(e)}")
            raise
    
    @property
    def llm(self) -> BaseChatModel:
        """Get the LangChain LLM instance"""
        if self._llm is None:
            self._initialize_llm()
        return self._llm
    
    def get_llm_for_task(self, task_type: str = "default") -> BaseChatModel:
        """Get LLM configured for specific tasks"""
        # You can create different configurations for different tasks
        if task_type == "reasoning":
            return AzureChatOpenAI(
                azure_endpoint=self.settings.azure_openai_endpoint,
                api_key=self.settings.azure_openai_api_key,
                api_version=self.settings.azure_openai_api_version,
                deployment_name=self.settings.azure_openai_deployment_name,
                temperature=0.0,  # More deterministic for reasoning
                max_tokens=6000,  # Longer responses for complex reasoning
            )
        elif task_type == "creative":
            return AzureChatOpenAI(
                azure_endpoint=self.settings.azure_openai_endpoint,
                api_key=self.settings.azure_openai_api_key,
                api_version=self.settings.azure_openai_api_version,
                deployment_name=self.settings.azure_openai_deployment_name,
                temperature=0.7,  # More creative for explanations
                max_tokens=3000,
            )
        
        return self.llm

# Global LLM manager instance
_llm_manager = None

def get_llm_manager() -> LLMManager:
    """Get the global LLM manager instance"""
    global _llm_manager
    if _llm_manager is None:
        _llm_manager = LLMManager()
    return _llm_manager

def get_llm() -> BaseChatModel:
    """Quick access to the default LLM"""
    return get_llm_manager().llm